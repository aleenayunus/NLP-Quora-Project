{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Embedding - Paragram\n",
    "This notebook uses the paragram pretrained embeddings for word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "# import re\n",
    "import string\n",
    "import contractions\n",
    "import os\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "import inflect\n",
    "# os.chdir('/home/rinkita/courses/NLPSeminar/nlp-quora-kaggle')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragram_word_embedding_path = \"./data/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "PARA_SIZE = 300\n",
    "embedding_size=64\n",
    "num_class=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/balanced_train.csv\")\n",
    "x_train=data.question_text\n",
    "max_document_len=x_train.map(lambda x: len(x.split(' '))).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161620/161620 [00:36<00:00, 4404.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"Replace contractions in string of text\"\"\"\n",
    "contracted_input = x_train.progress_apply(lambda row:contractions.fix(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(question_text):\n",
    "    question_text = question_text.translate(str.maketrans('','',string.punctuation))\n",
    "    question_text = question_text.translate(str.maketrans('','','1234567890'))\n",
    "    question_text = \" \".join(question_text.split())    \n",
    "    question_text = question_text.lower().split()\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # question_text = [w for w in question_text if not w in stop_words and len(w) >= 3]\n",
    "    question_text = [w for w in question_text if len(w) >= 3]\n",
    "    question_text = [w.strip('/\"“”') for w in question_text ]\n",
    "    question_text = \" \".join(question_text)\n",
    "    return question_text\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_features(question_texts):  \n",
    "    word2index = {}\n",
    "    word2count = {}\n",
    "    index2word = {0: \"pad\"}\n",
    "    n_words = 1  # Count SOS and EOS\n",
    "    for text in question_texts:\n",
    "        for word in text:             \n",
    "            if word not in word2index:\n",
    "                word2index[word] = n_words\n",
    "                word2count[word] = 1\n",
    "                index2word[n_words] = word\n",
    "                n_words += 1\n",
    "            else:\n",
    "                word2count[word] += 1\n",
    "    return word2index,word2count,index2word,n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 161620/161620 [00:02<00:00, 56697.20it/s]\n",
      "100%|██████████| 161620/161620 [00:35<00:00, 4534.89it/s]\n",
      "100%|██████████| 161620/161620 [00:14<00:00, 11254.81it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = contracted_input.progress_apply(lambda row:clean_text(row))\n",
    "\n",
    "word_tokenised = cleaned_text.progress_apply(lambda text:nltk.word_tokenize(text))\n",
    "\n",
    "lemmatized_words = word_tokenised.progress_apply(lambda word_list:lemmatize_verbs(word_list))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index,word2count,index2word,n_words = extract_features(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_para(path_to_paragram,word2index_map):\n",
    "    embedding_weights = {}\n",
    "    count_all_words = 0\n",
    "    with open(path_to_paragram,'r', encoding=\"utf8\", errors=\"ignore\") as f:  \n",
    "        for line in f:\n",
    "            vals = line.split(' ')\n",
    "            word = str(vals[0])\n",
    "            if word in word2index_map:                \n",
    "                count_all_words += 1                                 \n",
    "                coefs = np.asarray(vals[1:],dtype='float32')\n",
    "                coefs /= np.linalg.norm(coefs)\n",
    "                embedding_weights[word] = coefs\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " word2embedding_dict = get_para(paragram_word_embedding_path,word2index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((n_words,PARA_SIZE))\n",
    "\n",
    "out_of_vocabulary = {}\n",
    "\n",
    "embeddig_available = 0\n",
    "for word,index  in word2index.items():\n",
    "    try:        \n",
    "        word_embedding = word2embedding_dict[word]\n",
    "        embedding_matrix[index,:] = word_embedding\n",
    "    except:\n",
    "        out_of_vocabulary[word] = word2index[word]\n",
    "        embedding_matrix[index,:] = [0.0]* PARA_SIZE\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size is 60617\n",
      "no. of words for which no pre-trained embedding found is 13660\n",
      "no. of words for which pre-trained embedding is available46957\n",
      "embedding found for 77.46506755530626 percentage of vocab\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size is {}\".format(n_words))\n",
    "print(\"no. of words for which no pre-trained embedding found is {}\".format(len(out_of_vocabulary)))\n",
    "print(\"no. of words for which pre-trained embedding is available{}\".format(n_words-len(out_of_vocabulary)))\n",
    "print(\"embedding found for {} percentage of vocab\".format(((n_words-len(out_of_vocabulary))/n_words)*100))\n",
    "# out_of_vocabulary\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
