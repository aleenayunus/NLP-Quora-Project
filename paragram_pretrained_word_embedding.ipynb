{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "# import re\n",
    "import string\n",
    "import contractions\n",
    "import os\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "import inflect\n",
    "# os.chdir('/home/rinkita/courses/NLPSeminar/nlp-quora-kaggle')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragram_word_embedding_path = \"./data/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "PARA_SIZE = 300\n",
    "embedding_size=64\n",
    "num_class=2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/train.csv\")\n",
    "x_train=data.question_text\n",
    "max_document_len=x_train.map(lambda x: len(x.split(' '))).max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [03:47<00:00, 5751.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"Replace contractions in string of text\"\"\"\n",
    "contracted_input = x_train.progress_apply(lambda row:contractions.fix(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(question_text):\n",
    "    question_text = question_text.translate(str.maketrans('','',string.punctuation))\n",
    "    question_text = question_text.translate(str.maketrans('','','1234567890'))\n",
    "    question_text = \" \".join(question_text.split())    \n",
    "    question_text = question_text.lower().split()\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # question_text = [w for w in question_text if not w in stop_words and len(w) >= 3]\n",
    "    question_text = [w for w in question_text if len(w) >= 3]\n",
    "    question_text = [w.strip('/\"“”') for w in question_text ]\n",
    "    question_text = \" \".join(question_text)\n",
    "    return question_text\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def extract_features(question_texts):  \n",
    "    word2index = {}\n",
    "    word2count = {}\n",
    "    index2word = {0: \"pad\"}\n",
    "    n_words = 1  # Count SOS and EOS\n",
    "    for text in question_texts:\n",
    "        for word in text:             \n",
    "            if word not in word2index:\n",
    "                word2index[word] = n_words\n",
    "                word2count[word] = 1\n",
    "                index2word[n_words] = word\n",
    "                n_words += 1\n",
    "            else:\n",
    "                word2count[word] += 1\n",
    "    return word2index,word2count,index2word,n_words\n",
    "\n",
    "    \n",
    "\n",
    "#make all the input text of the same size as size max_length input senetnce, padding with word \"PAD\"(zero padding)\n",
    "def make_input(sentence):\n",
    "    sent_len = len(sentence.split(' '))\n",
    "    if sent_len < max_document_len:\n",
    "        padded_sentence = sentence + (max_document_len - len(sentence.split(' '))) * \" PAD\"\n",
    "    else:\n",
    "        padded_sentence = sentence\n",
    "\n",
    "    return padded_sentence,sent_len\n",
    "\n",
    "\n",
    "#function to return batch of data\n",
    "def get_sentence_batch(batch_size,data_x,data_y,data_seqlens,input_metadata):\n",
    "    \n",
    "    #shuffling and creating training batch data of batch_size\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    \n",
    "    #converting sentence to index vector using word2index dictionary\n",
    "    x = [[input_metadata.word2index[word] for word in data_x[i].lower().split(' ')]for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1306122/1306122 [00:20<00:00, 62568.42it/s]\n",
      "100%|██████████| 1306122/1306122 [04:06<00:00, 5299.32it/s]\n",
      "100%|██████████| 1306122/1306122 [01:16<00:00, 16987.00it/s]\n"
     ]
    }
   ],
   "source": [
    "cleaned_text = contracted_input.progress_apply(lambda row:clean_text(row))\n",
    "\n",
    "word_tokenised = cleaned_text.progress_apply(lambda text:nltk.word_tokenize(text))\n",
    "\n",
    "lemmatized_words = word_tokenised.progress_apply(lambda word_list:lemmatize_verbs(word_list))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index,word2count,index2word,n_words = extract_features(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_para(path_to_paragram,word2index_map):\n",
    "    embedding_weights = {}\n",
    "    count_all_words = 0\n",
    "    with open(path_to_paragram,'r', encoding=\"utf8\", errors=\"ignore\") as f:  \n",
    "        for line in f:\n",
    "            vals = line.split(' ')\n",
    "            word = str(vals[0])\n",
    "            if word in word2index_map:                \n",
    "                count_all_words += 1                                 \n",
    "                coefs = np.asarray(vals[1:],dtype='float32')\n",
    "                coefs /= np.linalg.norm(coefs)\n",
    "                embedding_weights[word] = coefs\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " word2embedding_dict = get_para(paragram_word_embedding_path,word2index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((n_words,PARA_SIZE))\n",
    "\n",
    "out_of_vocabulary = {}\n",
    "\n",
    "embeddig_available = 0\n",
    "for word,index  in word2index.items():\n",
    "    try:        \n",
    "        word_embedding = word2embedding_dict[word]\n",
    "        embedding_matrix[index,:] = word_embedding\n",
    "    except:\n",
    "        out_of_vocabulary[word] = word2index[word]\n",
    "        embedding_matrix[index,:] = [0.0]* PARA_SIZE\n",
    "       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size is 216210\n",
      "no. of words for which no pre-trained embedding found is 88753\n",
      "no. of words for which pre-trained embedding is available127457\n",
      "embedding found for 58.95055732852319 percentage of vocab\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size is {}\".format(n_words))\n",
    "print(\"no. of words for which no pre-trained embedding found is {}\".format(len(out_of_vocabulary)))\n",
    "print(\"no. of words for which pre-trained embedding is available{}\".format(n_words-len(out_of_vocabulary)))\n",
    "print(\"embedding found for {} percentage of vocab\".format(((n_words-len(out_of_vocabulary))/n_words)*100))\n",
    "# out_of_vocabulary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make all the input text of the same size as size max_length input senetnce, padding with word \"PAD\"(zero padding)\n",
    "def make_input(sentence):\n",
    "    sent_len = len(sentence.split(' '))\n",
    "    if sent_len < max_document_len:\n",
    "        padded_sentence = sentence + (max_document_len - len(sentence.split(' '))) * \" PAD\"\n",
    "    else:\n",
    "        padded_sentence = sentence\n",
    "\n",
    "    return padded_sentence,sent_len\n",
    "\n",
    "\n",
    "#function to return batch of data\n",
    "def get_sentence_batch(batch_size,data_x,data_y,data_seqlens,input_metadata):\n",
    "    \n",
    "    #shuffling and creating training batch data of batch_size\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    \n",
    "    #converting sentence to index vector using word2index dictionary\n",
    "    x = [[input_metadata.word2index[word] for word in data_x[i].lower().split(' ')]for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
